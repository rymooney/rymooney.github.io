[
  {
    "objectID": "TextAnalysis.html",
    "href": "TextAnalysis.html",
    "title": "Text Analysis",
    "section": "",
    "text": "The Task\nToday, I will be practicing my text analysis using str_*() commands, regular expressions, and plotting character-based data! To do this, I will be analyzing character data from all of The New York Times’ headlines from years 1996-2006.\n\n\nThe Data\nThe data is sample dataset containing headlines from The New York Times, compiled by Professor Amber E. Boydstun at the University of California, Davis. The dataset can be accessed in the RTextTools library on R from the RTextTools package. The data is in the library in a data frame called NYTimes.\n\nlibrary(tidyverse)\nlibrary(RTextTools) \ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows\n\n\nThe libraries I will need:\n\nlibrary(tidyverse) \nlibrary(ggrepel)\nlibrary(stringr)\nlibrary(lubridate)\n\n\n\nLet’s analyze!\nLet’s see what years the dataset has.\n\nNYTimes_dates &lt;- NYTimes |&gt;\n  mutate(date_as_dmy = dmy(Date)) |&gt;\n  summarize(\n    earliestdate = min(date_as_dmy, na.rm = TRUE),\n    latestdate   = max(date_as_dmy, na.rm = TRUE)\n  )\n\nNYTimes_dates\n\n  earliestdate latestdate\n1   1996-01-01 2006-12-31\n\n\nFirst, I know that President Bush was a hot topic during these years.\nLet’s see the distribution of this over the years included in this dataset. I also want to exclude the mention of bush fires, which also took place and may have been talked about on NYT often, so I will reflect that in my regular expression using a negative look-forward!\n\nbush_mentioned &lt;- NYTimes |&gt;\n  mutate(Title_lower = str_to_lower(Title))|&gt;\n  mutate(date = dmy(Date))|&gt;\n  mutate(year = year(date))|&gt;\n  filter(str_detect(Title_lower, \"\\\\bbush\\\\b(?!\\\\s*fires)\")) |&gt;\n  count(year)\n\nbush_mentioned\n\n  year  n\n1 1999  2\n2 2000 15\n3 2001 32\n4 2002 20\n5 2003 16\n6 2004 20\n7 2005 13\n8 2006  8\n\nggplot(bush_mentioned, aes(x = year, y = n)) +\n  geom_line() +\n  labs(\n    title = \"Presence of 'Bush' in NYT Headlines yearly (1996–2006)\",\n    x = \"Year\",\n    y = \"summed count per year\"\n  ) \n\n\n\n\n\n\n\n\nThere was a peak in the mentions of “Bush” in the year 2001. This makes sense due to his inauguration as President of the United States and the September 11th both taking place that year.\nOn the topic of presidents, let’s quantify how much different presidents were talked about in the New York Times across the time frame represented by this data. I will use another regular expression to find the words “Bush”, “Clinton”, and “Obama”, excluding the search for “bush fire”, mentions of “Hillary Clinton”, and “Michelle Obama”, as for this analysis I am only looking for information on the presidents. To do this, I will make use of a couple of lookarounds, which are proving to be quite useful for specifying data!\n\npresident_names &lt;- NYTimes |&gt;\n  mutate(\n    titles_lower = str_to_lower(Title),\n    president = str_extract(\n      titles_lower,\n      \"(?&lt;!laura\\\\s)\\\\bbush(?!\\\\s*fires?)\\\\b|(?&lt;!hillary\\\\s)\\\\bclinton\\\\b|\n      (?&lt;!michelle\\\\s)\\\\bobama\\\\b\")) |&gt;\n  select(titles_lower, president) |&gt;\n  filter(!is.na(president))|&gt;\n  group_by(president)|&gt;\n  summarize(count=n())\n\n\npresident_names |&gt;\n  ggplot(aes(x = president, y = count)) +\n  geom_col()+\n  labs(\n    title = 'Mentions of \"Bush\" and \"Clinton\" in NYT Headlines (1996–2006)',\n    x = \"President\",\n    y = \"Count\"\n  ) \n\n\n\n\n\n\n\n\nSo, Bush was mentioned more than Clinton in this dataset from the years 1996 to 2006. Both were mentioned more than Obama, with 0 mentions.This makes sense as Obama only started running for election in 2007, right after this dataset ends. However, it is a bit surprising that he wasn’t mentioned at all prior to the year before his election. Curious!\nFinally, I am interested to see how the timing of the mentions of each of these presidents compares and if I can pull out historical dates from the peaks in the following plot.\n\npresidents_mentioned &lt;- NYTimes |&gt;\n  mutate(Title_lower = str_to_lower(Title)) |&gt;\n  mutate(date = dmy(Date)) |&gt;\n  mutate(year = year(date)) |&gt;\n  filter(str_detect(Title_lower, \"\\\\b(bush(?!\\\\s*fires?)|clinton)\\\\b\")) |&gt;\n  mutate(\n    president = ifelse(\n      str_detect(Title_lower, \"\\\\bbush(?!\\\\s*fires?)\\\\b\"),\n      \"Bush\", \"Clinton\")) |&gt;\n  group_by(year, president) |&gt;\n  summarize(n = n(), .groups = \"drop\")\n\nggplot(presidents_mentioned, aes(x = year, y = n, color = president)) +\n  geom_line() +\n  geom_point()+\n  scale_x_continuous(breaks = seq(1996, 2006, by = 1)) +\n  labs(\n    title = \"Presence of 'Bush' and 'Clinton' in NYT Headlines\",\n    subtitle= \"1996-2006\",\n    x = \"Year\",\n    y = \"Summed count per year\",\n    color = \"President\"\n  )\n\n\n\n\n\n\n\n\nWe can see that the mentions of “Clinton” peak in 1998, when he was impeached. Mentions of “Bush” begin rising shortly after, with this pattern starting to get mentioned in 1999, when his campaign for presidential election was occuring and leading to his victory in 2000. Bush was inaugurated in 2001 and that year is also when the September 11th attacks took place, so mentions of Bush were extremely high that year. There is another peak in 2004 in “Bush” mentions again, given his re-election as president.\nWe can use text data to see how much different topics are being talked about in the media, which is a very important thing to know, and it can be useful to identify specific periods of interest.\nAnd… that’s it for today!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi and welcome to my site!\n\nMy background\nI am originally from Denver, Colorado. I have three older sisters, who are some of my biggest inspirations. After graduating high school in 2023, I went to Pomona College, where I expect to graduate in 2027 with a Bachelor of Arts in Molecular Biology.\n\n\nMy research experience\nAt Pomona College, I have been involved in research since my freshman year. In Dr. Malkiat Johal’s lab, I work to quantify biomolecular interactions. I have worked on various projects leveraging surface chemistry instruments like Quartz Crystal Microgravimetry with Dissipation Monitoring (QCM-D) and Localized Surface Plasmon Resonance (LSPR). My work over my first-year was on the QCM-D, where we quantified off-target effects of beta-cyclodextrin drugs on biomimetic lipid membranes, which was published in Langmuir. I then switched gears over the summer and worked with the SPR to develop a novel method using chemical kinetics data to determine the molecular weight of polyelectrolytes (this work has been submitted, and we are waiting to hear back from the reviewers!). I am working on developing and finalizing some other projects related to enzyme inhibition and immuno-oncological interactions, so stay tuned for updates!\nI have also been fortunate enough to engage in research at Memorial Sloan Kettering Cancer Center under the guidance of Dr. Michael Gormally and Dr. Christopher Klebanoff. Here, I gained training in immuno-oncology and contributed to a project with the goals of designing immunotherapies for solid-phase cancers. Specifically, this project involved working with cell culture, ELISA, IncuCyte live-cell imaging, and flow cytometry to evaluate and quantify the killing of cancer cells by T cells transduced with cancer neoepitope-specific T cell receptors. I got to experiment with armoring our T cells with additional potency, in the form of constitutive cytokine (IL-18) secretion and by engineering CD8 co-receptor expression. Our results were tremendously promising, and in vivo trials of our work are beginning now!\nFor more information on my research experience, please see my Research Projects page."
  },
  {
    "objectID": "ResearchProjects.html",
    "href": "ResearchProjects.html",
    "title": "Research Projects",
    "section": "",
    "text": "As a research assisstant and independent research, I have had the chance to contribute to many research projects, which I will detail below! All the topics below are areas I have research experience in, culminating from two full years of academic research at Pomona College and one full-time summer at Memorial Sloan Kettering Cancer Center.\n\nCyclodextrin Drug Interactions with Lipid Membranes\n\n\n\nCyclodextrin abstract image\n\n\nEthan M. Fong, J. Sebastian D. Kinzie, Aaron Christopherson, Jacob K. Al-Hussieni, Kevin Ye, Ananya Vinay, Ryan Mooney, and Malkiat S. Johal, A Hydrophobic Goldilocks Zone for Cyclodextrin-Lipid-Membrane Interactions: Implications of Drug Hydrophobicity on Kinetics of Cholesterol Removal from Lipid Membranes, Langmuir, 2025, 41 (25), 15909–15917.\n\n\nValidation of protein-protein interactions\n\n\n\nProtein-protein interaction abstract image\n\n\nPaco, K., Paco-Mendivil, M., Zhang, Z., Zebardast, S., Mooney, R. M., Davila, C., Yang, T., Bassi, S., Olatayinbo, P., Gonzales, V., Bin Ashraf, F., Condori Roman, I., Le Roch, K., Johal, M.S., Tolstorukov, I., Hernandez, J., Barroso da Silva, F.L., Lonardi, S., Sazinsky, M.H., Ray, A. Fine-tuned Protein Language Model Identifies Antigen-specific B Cell Receptors from Immune Repertoires., bioRxiv, 2025. (In preparation for submission to Nature Biomedical Engineering.)\n\n\nPolyelectrolyte Molecular Weight Determination\n In preparation for Analytical Chemistry\n\n\nT cell Receptor-Public Neoantigen Binding\n\n\n\nNeoantigen Generation and Presentation"
  },
  {
    "objectID": "permutation.html",
    "href": "permutation.html",
    "title": "Permutation Test",
    "section": "",
    "text": "The Task\nToday, I will be running a permutation test using a data set from William Wolberg, Olvi Mangasarian. Nick Street, and W. Street in their paper “Nuclear feature extraction for breast tumor diagnosis” 1993 Published in Biomedical Image Processing and Biomedical Visualization.. The dataset was downloaded from UC Irvine Machine Learning Repository. The goal is to look at whether there is a correlation between diagnosis of a tumor and tumor size.\n\n\nThe Data\nThe data set compiled many features that were computed from a digitized image of a fine needle aspirate (FNA) of 529 breast masses. They recorded the following data:\n\nID number\nDiagnosis (M = malignant, B = benign)\n\nTen real-valued features are computed for each cell nucleus:\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 / area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry \nj) fractal dimension (\"coastline approximation\" - 1)\nThey provide standard errors and the “worst”, or the most extreme, value for each of the variables for each sample.\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(praise)\n\ntumor_data &lt;- read_csv(\"data/wdbc.data\")\n\ncolnames(tumor_data) &lt;- c(\n  \"id\", \"diagnosis\",\n  \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n  \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n  \n  \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\",\n  \"compactness_se\", \"concavity_se\", \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n  \n  \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",\n  \"compactness_worst\", \"concavity_worst\", \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\")\n\ntumor_data\n\n# A tibble: 568 × 32\n         id diagnosis radius_mean texture_mean perimeter_mean area_mean\n      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1   842517 M                20.6         17.8          133.      1326 \n 2 84300903 M                19.7         21.2          130       1203 \n 3 84348301 M                11.4         20.4           77.6      386.\n 4 84358402 M                20.3         14.3          135.      1297 \n 5   843786 M                12.4         15.7           82.6      477.\n 6   844359 M                18.2         20.0          120.      1040 \n 7 84458202 M                13.7         20.8           90.2      578.\n 8   844981 M                13           21.8           87.5      520.\n 9 84501001 M                12.5         24.0           84.0      476.\n10   845636 M                16.0         23.2          103.       798.\n# ℹ 558 more rows\n# ℹ 26 more variables: smoothness_mean &lt;dbl&gt;, compactness_mean &lt;dbl&gt;,\n#   concavity_mean &lt;dbl&gt;, concave_points_mean &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;,\n#   fractal_dimension_mean &lt;dbl&gt;, radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;,\n#   perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, smoothness_se &lt;dbl&gt;,\n#   compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, concave_points_se &lt;dbl&gt;,\n#   symmetry_se &lt;dbl&gt;, fractal_dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;, …\n\n\n\n\nThe Hypotheses\nToday, I will be investigating the relationship between two variables: diagnosis (whether the tumor is malignant or benign) and mean tumor cell area. These variables are interesting because malignancy and benign tumors have different characteristics: malignant tumors are cancerous, they grow uncontrollably, invade nearby tissues, and can spread to other parts of the body, and benign tumors are non-cancerous, slow-growing, don’t invade surrounding tissue, and are typically treatable or harmless. Looking at cell area (and other variables) is interesting because it could give insight into which features can best be used to predict malignancy from small samples of tumor cells. The null hypothesis is that benign tumors and malignant tumors have cells of the same average area. The alternative hypothesis is that malignant tumors have larger average cell areas.\nThe statistic to test this difference will be difference in means between area in the benign and malignant tumor samples.\n\ntumor_data |&gt; \n  group_by(diagnosis) |&gt; \n  summarize(ave_area = mean(area_mean))\n\n# A tibble: 2 × 2\n  diagnosis ave_area\n  &lt;chr&gt;        &lt;dbl&gt;\n1 B             463.\n2 M             978.\n\n\nHere is a visual of the original relationship in the data.\n\ntumor_data |&gt; \n  ggplot(aes(x = diagnosis, y = area_mean)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nSo, it looks like the mean area of malignant tumor cells is larger than that of benign tumor cells. However, is that generalizable to other breast tumors? Off to the permutation test!\n\n\nThe permutation test\nTo start, I will generate a null sample distribution to compare the observed data with. This will serve as the basis of the generation of the p-value. The map(c(1:1000)) component performs the perm_data function 1000 times to generate a solid null distribution.\n\nset.seed(47)\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(diagnosis, area_mean) |&gt;\n    mutate(area_perm = sample(area_mean, replace = FALSE)) |&gt;\n    group_by(diagnosis) |&gt;\n    summarize(\n      obs_mean  = mean(area_mean),\n      perm_mean = mean(area_perm)) |&gt;\n    summarize(\n      obs_mean_diff  = diff(obs_mean),\n      perm_mean_diff = diff(perm_mean),\n      rep = rep\n    )\n}\n\nmap(c(1:1000), perm_data, data = tumor_data) |&gt; \n  list_rbind()\n\n# A tibble: 1,000 × 3\n   obs_mean_diff perm_mean_diff   rep\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n 1          515.         30.9       1\n 2          515.        -52.8       2\n 3          515.         63.0       3\n 4          515.         24.5       4\n 5          515.         -4.64      5\n 6          515.          0.188     6\n 7          515.         -6.70      7\n 8          515.          9.85      8\n 9          515.         45.6       9\n10          515.        -55.7      10\n# ℹ 990 more rows\n\n\nLet’s look at the null sampling distribution and see where our experimental difference in means between malignant and benign tumor area lies.\n\nset.seed(47)\nperm_stats &lt;- \n  map(c(1:1000), perm_data, data = tumor_data) |&gt; \n  list_rbind() \n\nperm_stats |&gt; \n  ggplot(aes(x = perm_mean_diff)) + \n  geom_histogram(binwidth = 5) + \n  geom_vline(aes(xintercept = obs_mean_diff), color = \"red\") + \n  labs(\n    title = \"Null sampling distribution\",\n    subtitle = \"Red line is the observed difference in means\",\n    x = \"Mean Difference\"\n  )\n\n\n\n\n\n\n\n\n\nperm_stats |&gt; \n    summarize(p_val = mean(perm_mean_diff &gt; obs_mean_diff))\n\n# A tibble: 1 × 1\n  p_val\n  &lt;dbl&gt;\n1     0\n\n\nThe permutation test yielded a p-value of 0, indicating that the observed difference in mean cell size between malignant and benign breast tumors (515.479) did not occur once in 1,000 random permutations of the data. This extremely small p-value provides very strong evidence against the null hypothesis that benign and malignant tumors have the same average cell size. Thus, I claim that all malignant breast cancer cells have higher average sizes than benign cancer cells. This mean that average cell size could potentially serve as a potential quantitative metric for the rapid and automated classification of tumor malignancy.\n\n\nThat’s it!\nThanks for coming along for the ride today!\n\n\nReferences\nStreet, W.N., Wolberg, W.H., & Mangasarian, O.L. “Nuclear feature extraction for breast tumor diagnosis.” (1993) Proc. SPIE 1905: Biomedical Image Processing and Biomedical Visualization. https://doi.org/10.1117/12.148698\nWolberg, W., Mangasarian, O., Street, N., & Street, W. “Breast Cancer Wisconsin (Diagnostic)” (1993) UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B"
  },
  {
    "objectID": "pixar.html",
    "href": "pixar.html",
    "title": "pixar",
    "section": "",
    "text": "library(tidyverse)\n\n\nThe Data\nLet’s explore a data set about the ratings of pixar films Pixar films; the data here is from the TidyTuesday repo and the original data is from Wikipedia.\n\npixar_ratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')\n\n\n\nLet’s plot it!\n\npixar_ratings_long &lt;- pixar_ratings |&gt;\n  pivot_longer(\n    cols = c(rotten_tomatoes, metacritic, critics_choice),\n    names_to = \"rating_source\",\n    values_to = \"score\"\n  )|&gt;\n  filter(!is.na(score)) \n\nggplot(pixar_ratings_long, aes(x = film, y = score, color = rating_source)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = rating_source)) +\n  labs(\n    title = \"Pixar Movie Ratings\",\n    x = \"Film\",\n    y = \"Score\",\n    color = \"Rating Source\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\nReferences\n“List of Pixar Films.” Wikipedia, Wikimedia Foundation, 4 Dec. 2025, en.wikipedia.org/wiki/List_of_Pixar_films."
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Data Viz",
    "section": "",
    "text": "This is my data visualization page, where I upload my data visualizations done on TidyTuesday data."
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "SQL",
    "section": "",
    "text": "The task\nFor today’s project, I will be working with data from the Stanford Open Policing Project pubished by Pierson, et al. (2020). I will be investigating a few of the many tables contained within the database.\nFirst, let’s set up the connection to the traffic database and upload the needed packages for later analyses.\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nlibrary(tidyverse)\nlibrary(DBI)\n\nThere are many tables in the database, including three that I am particularly interested in. These data tables are called “ca_statewide_2023_01_26”, “co_statewide_2020_04_01”, and “ny_statewide_2020_04_01”. These tables have all the statewide data for California, Colorado, and New York. Let’s compare!\nTo get a sense of the kinds of variables included in each set, let’s look at the start of the California table.\n\nSELECT * FROM ca_statewide_2023_01_26 LIMIT 8;\n\n\n8 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_row_number\ndate\ncounty_name\ndistrict\nsubject_race\nsubject_sex\ndepartment_name\ntype\nviolation\narrest_made\ncitation_issued\nwarning_issued\noutcome\ncontraband_found\nfrisk_performed\nsearch_conducted\nsearch_person\nsearch_basis\nreason_for_stop\nraw_race\nraw_search_basis\nraw_location_code\n\n\n\n\n1\n2009-07-01\nStanislaus\nModesto\nother\nmale\nCalifornia Highway Patrol\nvehicular\nMotorist / Public Service\nNA\nNA\nNA\nNA\nNA\nNA\n0\n0\nNA\nMotorist / Public Service\nOther\nVehicle Inventory\n465\n\n\n2\n2009-07-01\nStanislaus\nModesto\nhispanic\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n3\n2009-07-01\nStanislaus\nModesto\nhispanic\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n1\nNA\nother\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n4\n2009-07-01\nStanislaus\nModesto\nwhite\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nWhite\nProbable Cause (positive)\n465\n\n\n5\n2009-07-01\nStanislaus\nModesto\nhispanic\nmale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n1\nNA\nother\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n6\n2009-07-01\nStanislaus\nModesto\nhispanic\nmale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n7\n2009-07-01\nStanislaus\nModesto\nhispanic\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n8\n2009-07-01\nStanislaus\nModesto\nother\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nOther\nProbable Cause (positive)\n465\n\n\n\n\n\nFirst, I’d like to make a plot to show how vehicular stops have changed over the years in California, Colorado, and New York.\nI am going to UNION ALL three tables (co_statewide, ca_statewide, and ny_statewide), so that I can have one large data set with the needed data for the three states. This way, I can do the wrangling on the three states seperately and use the UNION ALL to combine those wrangled tables. I am focused on only vehicular stops, so I will include a WHERE type = ‘vehicular’ statement. I will also GROUP BY state and year(date) so that I will have seperate count information for each year in each state. I will store the following wrangled data in a dataframe called stops_per_year. Let’s do it!\n\nSELECT\n    'Colorado' AS state,\n    YEAR(date) AS year,\n    COUNT(*) AS total_stops\nFROM co_statewide_2020_04_01\nWHERE type = 'vehicular'\nGROUP BY state, YEAR(date)\n\nUNION ALL\n\n SELECT\n    'California' AS state,\n    YEAR(date) AS year,\n    COUNT(*) AS total_stops\nFROM ca_statewide_2023_01_26\nWHERE type = 'vehicular'\nGROUP BY state, YEAR(date)\n\nUNION ALL\n\nSELECT\n    'New York' AS state,\n    YEAR(date) AS year,\n    COUNT(*) AS total_stops\nFROM ny_statewide_2020_04_01\nWHERE type = 'vehicular'\nGROUP BY state, YEAR(date)\n\nORDER BY year, state;\n\nNow we can visualize the data! Let’s see how the states compare when it comes to total stops per year!\n\nggplot(stops_per_year,\n       aes(x = year, y = total_stops, color = state)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Vehicular Stops in CA, CO, NY\",\n    x = \"Year\",\n    y = \"Stops\",\n    color = \"State\"\n  )\n\n\n\n\n\n\n\n\nSo, overall, California has more vehicular stops than Colorado and New York, and also has some more variation from year to year. Importantly, this is skewed upwards in the case of California because it has a much larger population!\nNext, now that I am more familiar with how these tables are organized and the specific variables available, I am interested in investigating California more closely to observe some more trends. I am interested how gender affects citation rates, as in how often do men get a citation when they are pulled over compared to women?\n\n\nSELECT\n    subject_sex,\n    COUNT(*) AS n_stops,\n    SUM(citation_issued) AS n_with_citation,\n    COUNT(*) - SUM(citation_issued) AS n_without_citation,\n    AVG(citation_issued) AS citation_rate\nFROM ca_statewide_2023_01_26\nWHERE subject_sex IN ('male', 'female')\nGROUP BY subject_sex\nORDER BY subject_sex;\n\nLet’s plot it!\n\nggplot(ca_sex_citations,\n       aes(x = subject_sex, y = citation_rate)) +\n  geom_col() +\n  geom_text(\n    aes(label = citation_rate),\n    vjust = -0.5)+\n  labs(\n    title = \"Citation Rate by Driver Sex in California\",\n    x = \"Driver Sex\",\n    y = \"Citation Rate\"\n  )\n\n\n\n\n\n\n\n\nFrom the above analysis, it is seen that males get citations when they are stopped more often than females. There are likely many variables going into this rate difference. This could be due to men being more aggressive or impulsive drivers. I was interested and did some quick research on this, and it has been found that younger drivers, those with powerful cars, and those with “macho” personalities are more likely to drive agressively (Krahé & Fenske, 2001). This lines up to my calculation of California citation rates. Interesting!\nFrom the analyses today, we can see that California has higher vehicular stops over time than Colorado and New York, and that males have higher citation rates than females in California. There are many, many variables that go into these kinds of analyses. These variables include population size, in the case of the first analysis, differences in driving behavior across states, and likely more social-related perception variables in the case of the difference in citation rates between men and women.\n\ndbDisconnect(con_traffic, shutdown = TRUE)\n\nThanks for coming along for the ride today!\n\n\nReferences\nKrahé, B., & Fenske, I. Predicting aggressive driving behavior: The role of macho personality, age, and power of car. Aggressive Behavior: Official Journal of the International Society for Research on Aggression, 28(1), 21-29. (2002).\nPierson, E., Simoiu, C., Overgoor, J. et al. A large-scale analysis of racial disparities in police stops across the United States. Nat Hum Behav 4, 736–745 (2020)."
  },
  {
    "objectID": "UNESCO.html",
    "href": "UNESCO.html",
    "title": "UNESCO",
    "section": "",
    "text": "library(tidyverse)\n\n\nThe Data\nI wanted to explore a simple data set comparing UNESCO world heritage sites in Norway, Denmark, and Sweden. The original data is from UNESCO and was downloaded from TidyTuesday. The data can be accessed from the {pixarfilms} R package by Eric Leung.\n\nunesco &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-06/heritage.csv')\n\n\nunesco2 &lt;- unesco |&gt;\n  pivot_longer(\n    cols = c(`2004`, `2022`),\n    names_to = \"year\",\n    values_to = \"number_of_sites\"\n  )\n\n\n\nLet’s plot the data!\n\nggplot(unesco2, aes(x = country, y = number_of_sites, color = year)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"UNESCO World Heritage Sites\",\n    x = \"Country\",\n    y = \"Number of Sites\"\n  )\n\n\n\n\n\n\n\n\nShowing the number of World Heritage Sites in Norway, Denmark, and Sweden across 2004 to 2022.\n#References UNESCO World Heritage Centre - World Heritage List, whc.unesco.org/en/list/."
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "The Task\nToday, my goal is to simulate probabilities related to the accumulation of genetic mutations in somatic cells, which are key drivers of oncogenesis. The mutations being modeled arise from spontaneous events in cells and environmental conditions. I will be modeling these mutations from a single cell the undergoes n mitotic divisions and yields a final cell lineage. At each division, I will assume there is an equal probability, prob, that a mutation occurs. Once any mutation happens, the lineage is considered “mutated” and is no longer considered a wild type population. We want to estimate the probability that the lineage has at least 1 mutation after n divisions.\n\n\nBeginning the model\nTo begin, I will set up a function to detect mutations. To do this, I will write a function with two inputs, one being the number of divisions, n_divisions, that we can model the cell dividing n times, and the second being prob, the probability of a mutation occuring at each division. For each division, I will generate a random uniform number between 0 and 1 with runif(). Then, I will compare this number with the probability value, prob, and say that there was a mutation if the random uniform number generated at that division is less than the probability that there is a mutation at that division. This will create a logical vector. If the sum of this logical vector is greater than 0, then there is a mutation in that simulated lineage.\n\nlibrary(tidyverse)\nset.seed(4747)\n\nhas_mutation &lt;- function(n_divisions, prob){\n  random &lt;- runif(n_divisions)\n  mutation_flag &lt;- random &lt; prob\n  total_mutations &lt;- sum(mutation_flag)\n  ifelse(total_mutations &gt; 0, TRUE, FALSE)\n}\n\nThe above logic can be used to simulate larger numbers of lineages. To start, we will simulate the probability of 0.001, meaning that each cell division has a 0.1% chance of generating a new mutation that persists in the lineage. I will simulate 10000 starting cells independently going through 50 cell divisions. The first map will repeat the next map 50 times, simulating each division. The map_lgl() simulates n_cells cells going through a division and repeats that process 10000 times, and sums the has_mutation() function defined above on each division This inner map_lgl() function returns TRUE if a lineage picked up at least one mutation, and returns FALSE otherwise. The combinations of the maps, while confusing at first, was the only way I figured out how to get multiple simulated cells going through mutliple division cycles. The final tibble() summarizes the results for each of the 10000 cells after each division cycle, and calculates the proportion of TRUE values after that division cycle. This gives a 2x1 data frame for each division. The final pipe into list_rbind() gives us a final data frame of the proportion of lineages that have a mutation after each division.\n\nset.seed(4747)\nprob_0 &lt;- 0.001\nn_cells &lt;- 10000\nnumber_of_divisions &lt;- 1:50\n\nresults &lt;- map(number_of_divisions, function(n) {\n  outcomes &lt;- map_lgl(1:n_cells, .f = ~ has_mutation(n_divisions = n, prob = prob_0))\n  tibble(divisions = n, prob_mutated = mean(outcomes))\n  }) |&gt; \n  list_rbind()\n\nresults\n\n# A tibble: 50 × 2\n   divisions prob_mutated\n       &lt;int&gt;        &lt;dbl&gt;\n 1         1       0.0017\n 2         2       0.0019\n 3         3       0.0028\n 4         4       0.0037\n 5         5       0.005 \n 6         6       0.0062\n 7         7       0.0063\n 8         8       0.0072\n 9         9       0.01  \n10        10       0.0086\n# ℹ 40 more rows\n\n\n\n\nLet’s visualize!\nLet’s visualize what this data frame looks like graphically as mutations are accumulated across the simulated cell populations.\n\nggplot(results, aes(x = divisions, y = prob_mutated * 100)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 100)) +\n  labs(\n    title = \"Probability a cell lineage has at least one mutation\",\n    subtitle = \"Mutation probability of 0.1% per division for 10,000 starting cells\",\n    x = \"Number of mitotic divisions\",\n    y = \"Percent of lineages with at least 1 mutation\"\n  )\n\n\n\n\n\n\n\n\nLet’s check how this plot changes with a a higher frequency of mutation, say a 1% chance of a mutation each cell division.\n\nset.seed(4747)\nprob_1 &lt;- 0.01\nn_cells &lt;- 10000\nnumber_of_divisions &lt;- 1:50\n\nresults_1 &lt;- map(number_of_divisions, function(n) {\n  outcomes &lt;- map_lgl(1:n_cells, .f = ~ has_mutation(n_divisions = n, prob = prob_1))\n  tibble(divisions_1 = n, prob_mutated_1 = mean(outcomes))\n  }) |&gt; \n  list_rbind()\n\nresults\n\n# A tibble: 50 × 2\n   divisions prob_mutated\n       &lt;int&gt;        &lt;dbl&gt;\n 1         1       0.0017\n 2         2       0.0019\n 3         3       0.0028\n 4         4       0.0037\n 5         5       0.005 \n 6         6       0.0062\n 7         7       0.0063\n 8         8       0.0072\n 9         9       0.01  \n10        10       0.0086\n# ℹ 40 more rows\n\nresults_1\n\n# A tibble: 50 × 2\n   divisions_1 prob_mutated_1\n         &lt;int&gt;          &lt;dbl&gt;\n 1           1         0.0104\n 2           2         0.0204\n 3           3         0.0296\n 4           4         0.0379\n 5           5         0.0481\n 6           6         0.0566\n 7           7         0.0696\n 8           8         0.0764\n 9           9         0.0876\n10          10         0.0911\n# ℹ 40 more rows\n\n\n\nggplot(results_1, aes(x = divisions_1, y = prob_mutated_1 * 100)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 100)) +\n  labs(\n    title = \"Probability a cell lineage has at least one mutation\",\n    subtitle = \"Mutation probability of 1% per division for 10000 starting cells\",\n    x = \"Number of mitotic divisions\",\n    y = \"Percent of lineages with at least 1 mutation\"\n  )\n\n\n\n\n\n\n\n\nWow. That’s a lot of accumulated mutations! Luckily, our cells do not have a 1% chance of accumulating a meaningful, harmful, and disruptive mutation each time they divide (if they did, we likely wouldn’t exist). Let’s use a more biologically relevant proportion of mutation.\nI’ll model a 1e-6 per-division probability that a daughter cell acquires a mutation that is not just silent, but actually alters fitness in a biologically meaningful way. This is a reasonable number, modeling a mutation in any meaningful, harmful mutation across a panel of key genes/pathways.\n\nset.seed(4747)\nprob_2 &lt;- 1e-6\nn_cells &lt;- 10000\nnumber_of_divisions &lt;- 1:50\n\nresults_2 &lt;- map(number_of_divisions, function(n) {\n  outcomes &lt;- map_lgl(1:n_cells, .f = ~ has_mutation(n_divisions = n, prob = prob_2))\n  tibble(divisions_2 = n, prob_mutated_2 = mean(outcomes))\n  }) |&gt; \n  list_rbind()\n\n\nresults_2\n\n# A tibble: 50 × 2\n   divisions_2 prob_mutated_2\n         &lt;int&gt;          &lt;dbl&gt;\n 1           1              0\n 2           2              0\n 3           3              0\n 4           4              0\n 5           5              0\n 6           6              0\n 7           7              0\n 8           8              0\n 9           9              0\n10          10              0\n# ℹ 40 more rows\n\n\nLooking at the above data frame, we know these mutation frequencies will be close to zero. So, I will zoom in on the graph to see the actual trend (i.e, the y-axis here will not go from 0 to 100).\n\nggplot(results_2, aes(x = divisions_2, y = prob_mutated_2)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Probability a cell lineage has at least one mutation\",\n    subtitle = \"Mutation probability of 0.0001% per division for 10000 starting cells\",\n    x = \"Number of mitotic divisions\",\n    y = \"Percent of lineages with at least 1 mutation\"\n  )\n\n\n\n\n\n\n\n\nThis is very close to 0. And that’s a good thing! Most cell divisions faithfully copy DNA with no meaningful changes. Once in tens of thousands or millions of divisions, a driver mutation pops up — a mutation that actually changes cell fitness and can lead to downstream problems. That rare event can seed a clone that can later expand, but the initial occurrence is tremendously rare, as shown above.\n\n\nThat’s it!\nThanks for joining me on my first simulation today! That was a lot of fun."
  },
  {
    "objectID": "tidytuesday10_28.html",
    "href": "tidytuesday10_28.html",
    "title": "Selected Literary Prizes",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(praise)\nlibrary(shiny)\nlibrary(tidytuesdayR)\nlibrary(ggalluvial)\n#| message: false\n#| warning: false\n\nprizes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv') |&gt; \n  mutate(highest_degree = ifelse(is.na(highest_degree), \"unknown\", \n                                 highest_degree)) |&gt; \n  mutate(highest_degree = forcats::fct_recode(highest_degree,  \n                                 \"a none\" = \"none\", \"b unknown\" = \"unknown\", \n                                 \"c Diploma\" = \"Diploma\", \n                                 \"d Certificate of Education\" = \"Certificate of Education\", \n                                 \"e Bachelors\" = \"Bachelors\", \n                                 \"f Masters\" = \"Masters\", \n                                 \"g Juris Doctor\" = \"Juris Doctor\", \n                                 \"h MD\" = \"MD\", \n                                 \"i Doctorate\" = \"Doctorate\",\n                                 \"j Postgraduate\" = \"Postgraduate\"))\n\nRows: 952 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): prize_alias, prize_name, prize_institution, prize_genre, person_id...\ndbl  (2): prize_id, prize_year\nlgl  (1): uk_residence\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe are going to make a sankey plot.\n\nbooker &lt;- prizes |&gt;\n  filter(prize_name == \"Booker Prize\") |&gt;\n  ggsankey::make_long(gender, ethnicity_macro, highest_degree) \n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\nbooker\n\n# A tibble: 249 × 4\n   x               node          next_x          next_node    \n   &lt;fct&gt;           &lt;chr&gt;         &lt;fct&gt;           &lt;chr&gt;        \n 1 gender          man           ethnicity_macro White British\n 2 ethnicity_macro White British highest_degree  e Bachelors  \n 3 highest_degree  e Bachelors   &lt;NA&gt;            &lt;NA&gt;         \n 4 gender          man           ethnicity_macro Irish        \n 5 ethnicity_macro Irish         highest_degree  e Bachelors  \n 6 highest_degree  e Bachelors   &lt;NA&gt;            &lt;NA&gt;         \n 7 gender          man           ethnicity_macro Asian        \n 8 ethnicity_macro Asian         highest_degree  e Bachelors  \n 9 highest_degree  e Bachelors   &lt;NA&gt;            &lt;NA&gt;         \n10 gender          man           ethnicity_macro Asian        \n# ℹ 239 more rows\n\n\n\nlibrary(ggsankey)\n  \np1 &lt;- ggplot(booker, aes(x = x, next_x = next_x, \n                node = node, next_node = next_node, \n                fill = factor(node), label = node)) +\n  geom_sankey(flow.alpha = 0.6, node.color = \"gray30\") +\n  geom_sankey_label(size = 3, color = \"white\", fill = \"gray40\") +\n  scale_fill_viridis_d() +\n  theme_sankey(base_size = 18) +\n  labs(x = NULL,\n       title = \"Booker Prize\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = .5))\n\n\nbailey &lt;- prizes |&gt;\n  filter(prize_name == \"Baillie Gifford Prize for Non-Fiction\") |&gt;\n  ggsankey::make_long(gender, ethnicity_macro, highest_degree) \n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\nbailey\n\n# A tibble: 423 × 4\n   x               node          next_x          next_node    \n   &lt;fct&gt;           &lt;chr&gt;         &lt;fct&gt;           &lt;chr&gt;        \n 1 gender          man           ethnicity_macro White British\n 2 ethnicity_macro White British highest_degree  b unknown    \n 3 highest_degree  b unknown     &lt;NA&gt;            &lt;NA&gt;         \n 4 gender          man           ethnicity_macro Jewish       \n 5 ethnicity_macro Jewish        highest_degree  b unknown    \n 6 highest_degree  b unknown     &lt;NA&gt;            &lt;NA&gt;         \n 7 gender          man           ethnicity_macro White British\n 8 ethnicity_macro White British highest_degree  e Bachelors  \n 9 highest_degree  e Bachelors   &lt;NA&gt;            &lt;NA&gt;         \n10 gender          man           ethnicity_macro White British\n# ℹ 413 more rows\n\n\n\np2 &lt;- ggplot(bailey, aes(x = x, next_x = next_x, \n                node = node, next_node = next_node, \n                fill = factor(node), label = node)) +\n  geom_sankey(flow.alpha = 0.6, node.color = \"gray30\") +\n  geom_sankey_label(size = 3, color = \"white\", fill = \"gray40\") +\n  scale_fill_viridis_d() +\n  theme_sankey(base_size = 18) +\n  labs(x = NULL,\n       title = \"Baillie Gifford Prize for Non-Fiction\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = .5))\n\n\njames &lt;- prizes |&gt;\n  filter(prize_name == \"James Tait Black Prize for Fiction\") |&gt;\n  ggsankey::make_long(gender, ethnicity_macro, highest_degree) \n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\np3 &lt;- ggplot(james, aes(x = x, next_x = next_x, \n                node = node, next_node = next_node, \n                fill = factor(node), label = node)) +\n  geom_sankey(flow.alpha = 0.6, node.color = \"gray30\") +\n  geom_sankey_label(size = 3, color = \"white\", fill = \"gray40\") +\n  scale_fill_viridis_d() +\n  theme_sankey(base_size = 18) +\n  labs(x = NULL,\n       title = \"James Tait Black Prize for Fiction\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = .5))\n\n\nscience &lt;- prizes |&gt;\n  filter(prize_name == \"BSFA Award for Best Novel\") |&gt;\n  ggsankey::make_long(gender, ethnicity_macro, highest_degree) \n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\np4 &lt;- ggplot(science, aes(x = x, next_x = next_x, \n                node = node, next_node = next_node, \n                fill = factor(node), label = node)) +\n  geom_sankey(flow.alpha = 0.6, node.color = \"gray30\") +\n  geom_sankey_label(size = 3, color = \"white\", fill = \"gray40\") +\n  scale_fill_viridis_d() +\n  theme_sankey(base_size = 18) +\n  labs(x = NULL,\n       title = \"BSFA Award for Best Novel\") +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = .5))\n\n\np1\n\n\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\n\n\np3\n\n\n\n\n\n\n\n\n\np4"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Data Science Ethics",
    "section": "",
    "text": "The Task\nToday, I will be exploring an example of ethics and power in the data science context. Specifically, I will be looking into a specific case study. The ethical dilemma revolves around an event that happened in 2012, which is narrated and given a great overview in Duhigg, C. (2012), I will also include some information from Fung Institute for Engineering Leadership (2013).\n\n\nSo, what happened?\nDuhigg (2012) reported that Target’s analytics team (namely, statistician Andrew Pole) built an algorithm that tried to infer which shoppers were likely in their second trimester of pregnancy. Their goal was to be able to start advertising to to-be parents before they even had the child, as they saw that as an opening in the market that they could generate more profit if they could advertise to this population. Andrew Pole was hired by Target to “to identify those unique moments in consumers’ lives [like late-pregnancy] when their shopping habits become particularly flexible and the right advertisement or coupon would cause them to begin spending in new ways” (Duhigg, 2012). In moments of life change, consumers are more highly susceptible to influence by advertisements! Pole looked at the data in Target’s datatables and found that pregnant people buy more lotion, supplements, soap, and cotton balls. So, Pole and analytics team developed an algorithm to predict a person’s due date, so Target could send timely, pregnancy-related coupons exactly when they are most likely to buy these items. Pole applied this algorithm to every regular customer at Target and generated a list of tens of thousands of customers likely to be pregnant so that they could be sent pregnancy-specific coupons and advertisements. The ethical dilemma here is the collection of personal health and family data without explicit consent and acting on the information inferred for corporate gain. When does an analytics team or algorithm go too far?\n\n\nWhat is the permission structure for using the data? Was it followed?\nTarget’s privacy policy essentially says that you automatically consent to have data collected about you unless you explicitly opt out. This means that as long as you shop or interact with Target, you are considered to have consented to their data being used for advertising and analytics. This was done by using transactional data and shopping-habits data to build a pregnancy prediction model in Target, which would influence the ads and coupons a customer received. Target technically did not breach any legal terms as outlined in their privacy policy. But ethically speaking, there was not informed consent on behalf of the customer, since they had no real way of knowing that their data would be used to infer medical or reproductive information and target advertising and coupons to you.\n\n\nHow were the variables collected? Were they accurately recorded? Is there any missing data?\nTarget tracked each shopper using a unique Guest ID, which linked together various variables related to shoppers. Namely, they identified 25 products that were likely to be purchased more by people in their second semester of pregnancy. The purchasing data of these 25 specific products were analyzed together and made a “Pregnancy Prediction Score” for each consumer. When a customer had a high pregnancy prediction score, they were marked to receive advertisements and coupons mailed to their home, because of course Target has this data as well. The transactional data itself was likely accurately recorded, as Target has tables that hold all of its customer’s transactions, what they purchased so that they can do analyses like this one. However, applying those 25 specific products to generate a pregnancy prediction score defninitely is not perfectly accurate. For example, a person buying unscented lotion does not automatically ensure that they are pregnant. So, the algorithm used by Target, while definitely good at predicting pregnancy, is not perfectly accurate. It also makes you me ask the question about if it is weirder if Target knows a person is pregnant and that person actually is or if Target thinks someone is pregnant but in not. Why should Target get to speculate about this?\n\n\nPublicity of data\nIn the Target pregnancy-prediction case, the data were not made publicly available. Instead, the company treated them as proprietary, confidential commercial assets that they would use specifically for profit-gain of the company. In this case, it’s likely a good thing that the data were not made public, since the information they were inferring, pregnancy, is highly personal. It would have made it even worse if this inferred information was made public.\n\n\nConsent structure\nCustomers’ shopping activity served as the data source for Target’s data collection, and their continued use of Target’s services was treated as implicit consent under the company’s general privacy policy. There were no explicit consent forms, no sign-up for sensitive analytics, and no notifications that predictive models were being trained on their purchasing behavior. Customers were unaware that their data was being used for pregnancy prediction, and Target made additional efforts to keep that hidden, specifically by making the pregnancy related ads appear random, for example by putting advertisements for diapers right next to advertisements for lawn mowers, with the purpose of making consumers unaware that they were being targeted for pregnancy-related coupons (Duhigg 2012). The way that Target was intentionally trying to make the pregnancy-related ads appear random makes me think that they knew something was off with what they were doing.\n\n\nWhy does this matter?\nWho benefits? Target. In this case, Target aimed to exploit key turning points in people’s lives like having a child as an opportunity for profit. Also, analytics teams and people like Pole benefit from the prestige they gain for building “sophisticated” models and algorithms. Who is neglected/harmed? Consumers whose data was used for building the algorithm and those surveyed by the algorithm had their privacy invaded and personal health information inferred without their knowing consent. Target even attempted to disguise the pregnancy-related advertisements by mixing them with unrelated promotions, which shows an awareness that their actions might be perceived as intrusive. The Fung Institute warns of customer backlash when people “feel as if they are being observed a little too closely”. The Fung Institute also provides some more points specifically for corporations that are using data analytics in their business models, namely that “big data leadership requires judgment for ethical considerations and privacy”, something that Target may have overlooked.\nThe ethical violations in the Target case were absolutely done in the pursuit of increasing profit. The entire program was driven by the desire for profit by taking advantage of consumers in the times of their lives when they are most susceptible to advertisements. There was also an increase in surveillance in Target’s data analytics, as they were now collecting data not just for analysis on their business, but for health-related predictions of consumers lives. The resulting power imbalance, where companies infer personal and private details about individuals who remain unaware of the data being collected on them, emphasizes why this case remains so relevant today as algorithms grow increasingly powerful.\n\n\nThat’s it!\nSee you next time!\n\n\nReferences\n“Avoiding the Traps of Big Data.” Fung Institute for Engineering Leadership, 7 Apr. 2015, funginstitute.berkeley.edu/news/avoiding-the-traps-of-big-data/.\nDuhigg, Charles. “How Companies Learn Your Secrets.” NY Times Magazine, 16 Feb. 2012, www.nytimes.com/2012/02/19/magazine/shopping-habits.html.\n“Target Privacy Policy.” Target, www.target.com/c/target-privacy-policy/-/N-4sr7p."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Mooney",
    "section": "",
    "text": "Hi there! I am Ryan, a student at Pomona College majoring in Molecular Biology. I am an aspiring biomedical researcher, with keen interests in cancer biology and immuno-oncology. Sequencing has revolutionized how we analyze genetic information, so spending time developing skills in data science has provided me with invaluable skills to approach translational research questions. I love learning, and pursuing a career in research is how I plan to continue doing what I love (to learn!). I am a rather crafty person, and I particularly enjoy knitting, sewing, and collaging.\nPlease feel free to check out my CV.\nCheck out the pages on my site to learn more about me!"
  },
  {
    "objectID": "permutation_pres.html#section",
    "href": "permutation_pres.html#section",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "",
    "text": "So, it looks like the mean area of malignant tumor cells is larger than that of benign tumor cells. However, is that generalizable to other breast tumors? Off to the permutation test!"
  },
  {
    "objectID": "permutation_pres.html#the-data",
    "href": "permutation_pres.html#the-data",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "The data",
    "text": "The data\nThe data set is from William Wolberg et. al, (1993) in Biomedical Image Processing and Biomedical Visualization., which contains data on 568 breast cancer tumor samples. The data set was downloaded from UC Irvine Machine Learning Repository.\nNeeded packages:\n\nlibrary(tidyverse)\nlibrary(readr)"
  },
  {
    "objectID": "permutation_pres.html#what-does-the-data-look-like",
    "href": "permutation_pres.html#what-does-the-data-look-like",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "What does the data look like?",
    "text": "What does the data look like?\n\nhead(tumor_data)\n\n# A tibble: 6 × 32\n        id diagnosis radius_mean texture_mean perimeter_mean area_mean\n     &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1   842517 M                20.6         17.8          133.      1326 \n2 84300903 M                19.7         21.2          130       1203 \n3 84348301 M                11.4         20.4           77.6      386.\n4 84358402 M                20.3         14.3          135.      1297 \n5   843786 M                12.4         15.7           82.6      477.\n6   844359 M                18.2         20.0          120.      1040 \n# ℹ 26 more variables: smoothness_mean &lt;dbl&gt;, compactness_mean &lt;dbl&gt;,\n#   concavity_mean &lt;dbl&gt;, concave_points_mean &lt;dbl&gt;, symmetry_mean &lt;dbl&gt;,\n#   fractal_dimension_mean &lt;dbl&gt;, radius_se &lt;dbl&gt;, texture_se &lt;dbl&gt;,\n#   perimeter_se &lt;dbl&gt;, area_se &lt;dbl&gt;, smoothness_se &lt;dbl&gt;,\n#   compactness_se &lt;dbl&gt;, concavity_se &lt;dbl&gt;, concave_points_se &lt;dbl&gt;,\n#   symmetry_se &lt;dbl&gt;, fractal_dimension_se &lt;dbl&gt;, radius_worst &lt;dbl&gt;,\n#   texture_worst &lt;dbl&gt;, perimeter_worst &lt;dbl&gt;, area_worst &lt;dbl&gt;, …"
  },
  {
    "objectID": "permutation_pres.html#the-goal",
    "href": "permutation_pres.html#the-goal",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "The goal",
    "text": "The goal\nDetermine if there is a correlation between diagnosis of a tumor and tumor size.\nThe method to answer this question: permutation test!\nIf you are here from my website, and want to go back, please click here!"
  },
  {
    "objectID": "permutation_pres.html#hypotheses",
    "href": "permutation_pres.html#hypotheses",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe following hypotheses are rooted in the fact that malignant cancer cells go through the cell cycle faster, including the G1 and G2 phases, which are periods of cell growth. So, I hypothesize that malignant cancer cells in general are larger on average than benign cancer cells.\nThe null hypothesis: benign tumors and malignant tumors have cells of the same average area.\nThe alternative hypothesis: malignant tumors have larger average cell areas."
  },
  {
    "objectID": "permutation_pres.html#the-variables-of-interest-and-test-statistic",
    "href": "permutation_pres.html#the-variables-of-interest-and-test-statistic",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "The Variables of Interest and Test Statistic",
    "text": "The Variables of Interest and Test Statistic\nThe variables I will be investigating are diagnosis (whether the tumor is malignant or benign), designated by an ‘M’ or a ‘B’ in the diagnosis column, and mean tumor cell area, which a calculated value for each sample in the area_mean column.\nThe statistic to test the potential difference in cell area between benign and malignant tumors is the difference in means between area in the benign and malignant tumor samples."
  },
  {
    "objectID": "permutation_pres.html#lets-have-a-look-at-the-data",
    "href": "permutation_pres.html#lets-have-a-look-at-the-data",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "Let’s have a look at the data",
    "text": "Let’s have a look at the data\n\ntumor_data |&gt; \n  group_by(diagnosis) |&gt; \n  summarize(ave_area = mean(area_mean))\n\n# A tibble: 2 × 2\n  diagnosis ave_area\n  &lt;chr&gt;        &lt;dbl&gt;\n1 B             463.\n2 M             978."
  },
  {
    "objectID": "permutation_pres.html#the-permutation-test",
    "href": "permutation_pres.html#the-permutation-test",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "The Permutation Test",
    "text": "The Permutation Test\n\nset.seed(47)\nperm_data &lt;- function(rep, data) {\n  data |&gt;\n    select(diagnosis, area_mean) |&gt;\n    mutate(area_perm = sample(area_mean, replace = FALSE)) |&gt;\n    group_by(diagnosis) |&gt;\n    summarize(\n      obs_mean  = mean(area_mean),\n      perm_mean = mean(area_perm)) |&gt;\n    summarize(\n      obs_mean_diff  = diff(obs_mean),\n      perm_mean_diff = diff(perm_mean),\n      rep = rep\n    )\n}\n\nmap(c(1:1000), perm_data, data = tumor_data) |&gt; \n  list_rbind()\n\n# A tibble: 1,000 × 3\n   obs_mean_diff perm_mean_diff   rep\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n 1          515.         30.9       1\n 2          515.        -52.8       2\n 3          515.         63.0       3\n 4          515.         24.5       4\n 5          515.         -4.64      5\n 6          515.          0.188     6\n 7          515.         -6.70      7\n 8          515.          9.85      8\n 9          515.         45.6       9\n10          515.        -55.7      10\n# ℹ 990 more rows"
  },
  {
    "objectID": "permutation_pres.html#whats-the-p-value",
    "href": "permutation_pres.html#whats-the-p-value",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "What’s the p value?",
    "text": "What’s the p value?\n\nperm_stats |&gt; \n    summarize(p_val = mean(perm_mean_diff &gt; obs_mean_diff))\n\n# A tibble: 1 × 1\n  p_val\n  &lt;dbl&gt;\n1     0"
  },
  {
    "objectID": "permutation_pres.html#conclusions",
    "href": "permutation_pres.html#conclusions",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "Conclusions",
    "text": "Conclusions\nThe permutation test yielded a p-value of 0, indicating that the observed difference in mean cell size between malignant and benign breast tumors (515.479) did not occur once in 1,000 random permutations of the data. This extremely small p-value provides very strong evidence against the null hypothesis that benign and malignant tumors have the same average cell size. Thus, I claim that, in general, malignant breast cancer cells have higher average sizes than benign breast cancer cells."
  },
  {
    "objectID": "permutation_pres.html#visualizing-the-null-distribution",
    "href": "permutation_pres.html#visualizing-the-null-distribution",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "Visualizing the null distribution",
    "text": "Visualizing the null distribution"
  },
  {
    "objectID": "permutation_pres.html#references",
    "href": "permutation_pres.html#references",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "References",
    "text": "References\nStreet, W.N., Wolberg, W.H., & Mangasarian, O.L. “Nuclear feature extraction for breast tumor diagnosis.” (1993) Proc. SPIE 1905: Biomedical Image Processing and Biomedical Visualization. https://doi.org/10.1117/12.148698\nWolberg, W., Mangasarian, O., Street, N., & Street, W. “Breast Cancer Wisconsin (Diagnostic)” (1993) UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B"
  },
  {
    "objectID": "permutation_pres.html#lets-have-a-look-at-the-observed-test-statistic",
    "href": "permutation_pres.html#lets-have-a-look-at-the-observed-test-statistic",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "Let’s have a look at the observed test statistic",
    "text": "Let’s have a look at the observed test statistic\n\ntumor_data |&gt; \n  group_by(diagnosis) |&gt; \n  summarize(ave_area = mean(area_mean))\n\n# A tibble: 2 × 2\n  diagnosis ave_area\n  &lt;chr&gt;        &lt;dbl&gt;\n1 B             463.\n2 M             978."
  },
  {
    "objectID": "permutation_pres.html#implications",
    "href": "permutation_pres.html#implications",
    "title": "Are Malignant Cancer Cells Really Bigger?",
    "section": "Implications",
    "text": "Implications\nThis mean that average cell size could potentially serve as a potential quantitative metric for the rapid and automated classification of tumor malignancy."
  }
]